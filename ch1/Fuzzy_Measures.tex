% \signal{Possibility is normalized so that 1 membership is attained at some point. And it models the compatibility of 2 states. That is of being 1.80 height and being tall states.

% Probability Models aleatoric uncertainty y Possibility models epistemic uncertainty?\\

% Probability-Possibility Transformations:
% Under certain conditions, one can associate a family of probability distributions with a given possibility distribution. For instance, the possibility distribution can be seen as an upper envelope for a set of probability measures that are consistent with the available imprecise information. Such transformations (e.g., the Dubois-Prade method) allow one to move between the two representations, albeit at the cost of introducing conservatism or ambiguity.\\


% Bayesian Models for modeling both uncertainties:\\

% Epistemic Uncertainty:

% Bayesian models explicitly account for uncertainty about the model parameters by placing a prior over them and computing a posterior after observing data. This kind of uncertainty reflects our lack of knowledge due to limited data and can be reduced by gathering more information.\\

% Aleatoric Uncertainty:

% This type of uncertainty represents the inherent noise in the data itself (for example, measurement error). In Bayesian modeling, aleatoric uncertainty is often incorporated into the likelihood function. It is considered irreducible since it stems from the randomness in the data generation process.\\

% Osea que a qué nos referimos con epistemologic uncertainty? A que nos falta info (data) o a que tenemos toda la info que se puede tener y no podemos reducir ese gap de incetidumbre porque es un problema de definición imprecisa. Tengo que diferenciar entre vagueza y epistemologico entonces?

% \subsection{Probability-Possibility Transformations}
% %https://www.researchgate.net/publication/2743789_On_PossibilityProbability_Transformations
% From the paper: \cite{Dubois1997}: 
% it is recalled in this paper that the probabilistic representations and the possibilisticones are not just two equivalent representations of uncertainty

% The possibilisticrepresentation is weaker because it explicitly handles imprecision (e.g. incompleteknowledge) and because possibility measures are based on an \textbf{ordering structure} rather than an \textbf{additive one}.

% the principle of insufficient reason from possibility toprobability, and the principle of maximum specificity from probability topossibility.

% Mencionar: consistency principle, principle of insufficient reason and principle of maximum specificity
% }






% \signal{Y HABLAR TAMBIÉN DE PLAUSIBILITY MEASURE Y CÓMO ESO SE RELACIONA CON EL LAS OTRAS MEASURES. MIRA EN Joseph Y. Halpern - Reasoning about Uncertainty-The MIT Press (2003)}


The study of fuzzy measures emerged as a generalization of classical measure theory, motivated by the increasing realization that the additivity axiom, fundamental in traditional measurement and probability theories as formalized by Borel, Lebesgue, and Kolmogorov, was often ``too restrictive and, consequently, unrealistic" for modeling real-world complexities \cite[p.~10]{FuzzyMeasureHistory}. An alternative to classical measure theory came with Choquet's theory of capacities in 1954, followed by Dempster's and Shafer's development of belief and plausibility measures\footnote{Belief and plausibility measures are defined using a ``basic probability assignment", which distributes a total measure of evidence across subsets $A_i \,(i\in \mathcal{I}\subseteq \N)$ of outcomes. When these subsets are ``consonant" (they can be ordered with inclusion: $A_i \subseteq  A_{i+1}\forall i\in \mathcal{I}$), these measures become equivalent to possibility and necessity measures, respectively. This makes possibility and necessity special, highly structured cases within the broader Dempster-Shafer framework \cite[Thm.~3.23, Thm.~3.25]{FuzzyMeasureHistory}.} (superadditive and subadditive functions) in the late 1960s and 1970s to handle interval-valued probabilities. Simultaneously, the concept of a fuzzy set, introduced by Zadeh in 1965, laid the groundwork for possibility measures. It was within this context of fuzzy sets that Sugeno, in 1974, conceived fuzzy measures and fuzzy integrals, replacing the strict additivity requirement with weaker axioms of monotonicity and continuity, thereby paving the way for a more nuanced mathematical framework capable of capturing intrinsic nonadditive aspects of real-world phenomena like measurement errors and subjective judgments \cite[p.~13]{FuzzyMeasureHistory}.\\

\begin{definition}[Fuzzy Measure (Capacity)]
Let $\Omega$ be a universal set. A fuzzy measure (or capacity) $\nu$ is a set function
\[ \nu: 2^\Omega \to [0, 1] \]
that assigns a value to each subset of $\Omega$ (often called an event or criterion) such that:
\begin{romanenum}
    \item \textbf{Boundary Conditions:} The measure of the empty set is $0$ ($\nu(\emptyset) = 0$) and the measure of the universal set is $1$ ($\nu(\Omega) = 1$).

    \item \textbf{Monotonicity:} For any $A, B \subseteq \Omega$, if $A \subseteq B$, then $\nu(A) \le \nu(B)$.
\end{romanenum}
\end{definition}

The value $\nu(A)$ may quantify the weight, importance, degree of belief, or capacity associated with the subset $A$. A relevant aspect of any fuzzy measure $\nu$ defined over a finite universal set $\Omega$, is that it is fundamentally defined by the values assigned to all the subsets (i.e.\ $2^{|\Omega|}$ values in total, minus the two fixed boundary values). This implies that the greater generality of fuzzy measures comes at the cost of an exponentially growing number of parameters. By contrast, an additive measure only requires a linearly growing number of parameters $|\Omega|$. 
This is the motivation behind k-additive or $\lambda$-fuzzy measures (not further studied in this work), which employ parametric approaches to reduce the degrees of freedom, achieving polynomial complexity in the size of the universal set.\\


Fuzzy measures can be categorized according to various properties. For the purposes of this work, the discussion is focused on the additivity properties, and their formal definitions are presented as follows:


\begin{definition}[Additivity]
Given disjoint sets $A,B \subseteq N$ (i.e., $A \cap B = \emptyset$), a fuzzy measure $\nu$ is:
\begin{romanenum}
    \item \textbf{Subadditive} if $\nu(A \cup B) \leq \nu(A) + \nu(B)$ \cite[Def. 2.10, Eq. 2.8]{beliakov2023discrete}
    \item \textbf{Additive} if $\nu(A \cup B) = \nu(A) + \nu(B)$ \cite[Def. 2.4, Eq. 2.1]{beliakov2023discrete}
    \item \textbf{Superadditive} if $\nu(A \cup B) \geq \nu(A) + \nu(B)$ \cite[Def. 2.10, Eq. 2.9]{beliakov2023discrete}
\end{romanenum}
\end{definition}

\begin{example}[Additivity of probability and possibility measures]
    Probability measures are additive for disjoint sets $A,B$, satisfying $P(A \cup B) = P(A) + P(B)$. On the other hand,possibility measures, are maxitive, satisfying $Pos(A \cup B) = \max\{Pos(A), Pos(B)\}$ for any sets $A,B$ (a property that, for possibility measures, holds even for non-disjoint sets). The maxitivity of possibility measures directly implies they are subadditive, since $\max\{Pos(A), Pos(B)\} \leq Pos(A) + Pos(B)$ and measures are non-negative.
\end{example}


The crucial departure from the additivity requiremen allows fuzzy measures to model situations where, for example, the combined importance of two criteria $A$ and $B$ might be greater than the sum of their individual importances (synergy) or less (redundancy). 



\subsection{Probability and Possibility Measures}
Probability and possibility measures are two distinct yet related formalisms for quantifying uncertainty, both falling under the umbrella of fuzzy measures.

\begin{definition}[Probability Measure]
A probability measure $P$ is a fuzzy measure that additionally satisfies the axiom of finite additivity for disjoint sets:
For any two disjoint sets $A, B \subseteq \Omega$ (i.e., $A \cap B = \emptyset$):
\[ P(A \cup B) = P(A) + P(B) \]
\end{definition}
A probability measure quantifies the likelihood or long-term frequency of an event occurring. 


\begin{definition}[Possibility Measure]
A possibility measure $\Pi$, pioneered by Zadeh \cite{Zadeh1978}, is a fuzzy measure characterized by the axiom of maxitivity:
For any two sets $A, B \subseteq \Omega$:
\[ \Pi(A \cup B) = \max(\Pi(A), \Pi(B)) \]
\end{definition}
A possibility measure quantifies the degree to which an event is considered feasible (meaning possible, plausible, or compatible with available knowledge). \\

Both measures are typically derived from an underlying distribution. A probability measure is built from a probability mass function $p$ via summation, $P(A) = \sum_{x \in A} p(x)$, and is normalized such that $\sum_{x \in \Omega} p(x) = 1$. A possibility measure is built from a possibility distribution $\pi$ via the supremum, $\Pi(A) = \sup_{x \in A} \pi(x)$, and is normalized such that $\sup_{x \in \Omega} \pi(x) = 1$.\\


When Zadeh introduced possibility theory, he understood the possibility of an event as an upper bound on its probability. This is formalized by the \textbf{consistency principle}, which states that the probability of an event cannot exceed its possibility, i.e. $p(\omega) \leq \pi(\omega) \forall \omega \in \Omega$. This extends the idea of ``an unfeasible event must be impossible'', to a weak connection between probabilities and possibilities. In situations where the true probability $p$ is unknown, this principle is useful for defining a set of all probability distributions that are compatible with our current knowledge. 

An illustrative example is presented to facilitate the understanding of both probability and possibility measures, as well as the distinction between possibilities and memberships:


\begin{example}[Probabilities, Possibilities, and Memberships: \textit{An Ugly Die}]
    
    Imagine a six-sided die where each face, $\omega \in \Omega = \{1, 2, \dots, 6\}$, has a different painting on it. We want to model this situation using three distinct numerical scales:
    
    \begin{enumerate}
        \item \textbf{Probability $p(\omega)$}: This represents the objective, long-run frequency of an outcome. In general, we do not know the probability distribution of a random experiment, but let us assume for the example that the die is \textit{actually} fair, so the probability of any face appearing is uniform.
        
        \item \textbf{Feasibility Possibility $\pi_F(\omega)$}: This captures our \textit{prior belief} about the die's behavior. Before rolling it, we handle the die and get the impression that face 1 feels slightly heavier. On a standard die, the opposite face is 6, so we believe it is more feasible (i.e., more likely based on our current knowledge) for face 6 to land up. We also think faces 1 and 6 are more plausible outcomes than the others, which all seem equally possible. This expresses our subjective epistemic uncertainty.
        
        \item \textbf{Beauty Membership $\mu_B(\omega)$}: This represents our subjective \textit{preference} or aesthetic judgment. It quantifies the degree to which each face's painting belongs to our own concept of a ``beautiful face".
    \end{enumerate}
    
    These three distinct concepts are summarized in the table below:
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
        \textbf{Face ($\omega$)} & \textbf{Probability $p(\omega)$} & \textbf{Feasibility $\pi_F(\omega)$} & \textbf{Beauty $\mu_B(\omega)$} \\
        \hline
        1 & $1/6 \approx 0.167$ & 0.7 (Plausible) & 0.0 (Awful) \\
        2 & $1/6 \approx 0.167$ & 0.5 (Possible) & 0.0 (Awful) \\
        3 & $1/6 \approx 0.167$ & 0.5 (Possible) & 0.0 (Awful) \\
        4 & $1/6 \approx 0.167$ & 0.5 (Possible) & 0.0 (Awful) \\
        5 & $1/6 \approx 0.167$ & 0.5 (Possible) & 0.1 (A bit nice) \\
        6 & $1/6 \approx 0.167$ & 1.0 (Most feasible) & 1.0 (Beautiful) \\
    \end{tabular}
    \end{center}
    
    Then, \textit{what is the probability of getting a beautiful face?}
    
    To answer this, we must first define the event ``getting a beautiful face" as a crisp set over the set of outcomes, where our probability measure is defined. This crisp set depends on the \textbf{beauty membership} $\mu_B$. A natural way is to construct $B$ containing all faces that have a non-zero degree of beauty (any other desired threshold could be used):
    \[ B = \{\omega \in \Omega \mid \mu_B(\omega) > 0\} = \{5, 6\} \]
    The probability of this event is then calculated, as with any other event, by summing the probability masses of its elements:
    \[ P(B) = p(5) + p(6) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3} \]
    The only role of the membership function $\mu_B$ was to \textit{define the crisp event} $B$. The possibility distribution $\pi_F$ was entirely irrelevant for this specific calculation.
    
    \textit{Why use a possibility distribution at all?}

    As mentioned, a primary utility of a feasibility model like $\pi_F$ is to constrain the set of plausible probability distributions when the true probability is unknown, via the consistency principle $p(\omega) \leq \pi_F(\omega)$. However, this approach carries a significant risk: if the possibility distribution imposes bounds that are too restrictive (e.g., if we had mistakenly set $\pi_F(2)=0.1$), the resulting set of compatible probabilities could be completely unrealistic, or even exclude the true probability distribution altogether. For this reason, a conservative approach is generally preferred when modeling with possibilities. It is safer to define wider, less specific bounds to ensure that reality is not inadvertently ruled out by an overly confident or incorrect assumption.
\end{example}

A relevant question that arises from this example is: What is the difference between using a possibility or a subjective probability (e.g. Bayesian framework)? Two main distinctions can be identified: The possibility measure offers more expressiveness than a probability measure because it allows the representation of concepts such as absolute ignorance (where every subset has equal measure), which cannot be represented with an additive measure\cite{Dubois1997}. Additionally, a possibility measure do not specify the likelihood of any event (even though it can be used to derive a probability distribution, see the next subsection), making the direct update via Bayes rule illogical. Therefore, possibility distributions do not replace subjective probabilities, but rather provide a less restricted representation of our current knowledge from where a subjective probability could be derived.\\

\begin{remark}
    Not all the possibility distributions must be derived using only the subjective intuition of a decision maker as in the example. Data and more structured ``objective"\footnote{The objectivity of this approach is debatable since the selection of data and how it is processed can involve subjective choices despite being justifiable.} approaches could be used, or a mix of both. 
    For instance, the feasibility of a car making a sharp turn is based on physical limits: while completing the turn is fully possible ($\pi=1$) at 16~km/h, it becomes only partially possible at 64~km/h as the required centripetal force approaches the maximum friction of the tires, and physically impossible ($\pi=0$) at 97~km/h.
\end{remark}

Complementary to the possibility measure is its dual measure, called necessity measure, which quantifies the degree to which an event is not just feasible, but certain. The notion of necessity was not discussed in Zadeh's original paper but was subsequently introduced in the literature.

\begin{definition}[Necessity Measure]
A necessity measure $N$ is the dual of a possibility measure $\Pi$, and is defined for any set $A \subseteq \Omega$ by:
\[ N(A) = 1 - \Pi(A^c) \]
where $A^c$ is the complement of $A$.
\end{definition}

The intuition behind this formula can be understood by thinking about what it means for an event to be certain based on our knowledge. An event $A$ is absolutely certain only if its complement, $A^c$ (the event ``not A"), is absolutely impossible (unfeasible). If there is any feasibility to ``not A", our certainty in ``A" must be diminished.

The term $\Pi(A^c)$ precisely captures this ``plausibility of doubt". It is the highest degree of support our knowledge gives to any outcome outside of $A$. In the context of the consistency principle from Zadeh, necessity is interpreted as a lower bound for the probability distribution:
\[N(A) \le P(A) \le \Pi(A)\]

\subsection{Probability-Possibility Transformations}



The relationship $N(A) \le P(A) \le \Pi(A)$, provides the foundational link between probability and possibility. It establishes that a possibility measure $\Pi$ does not correspond to a single probability measure, but rather defines a family of consistent probability measures (also called a credal set $\mathcal{P}(\Pi)$) for which it serves as an upper bound.

Transformations between these formalisms are motivated by the need to bridge different forms of information. As explained by Dubois and Prade \cite{Dubois1997}, these transformations are not arbitrary but are guided by information principles. When moving from a precise probability to a less specific\footnote{Possibility distributions have greater expressive power while carrying less specific information. They are less restrictive than the probability distributions they bound, which makes them both more flexible in representation but less definitive in their implications.} possibility, the goal is to lose minimal information by constructing the tightest possible bounds. Conversely, when moving from a possibilistic model to a single probability measure, the goal is to add minimal new information by making the weakest, most non-committal assumption necessary.\\
The transformation formulas presented in this subsection are valid for the discrete case, the continuous case can also be found in \cite{Dubois1997}.

\subsubsection{From Probability to Possibility (P$\to\Pi$): The Principle of Maximum Specificity}
When converting a known probability distribution $p$ to a possibility distribution $\pi$, the objective is to find the most specific (or tightest) possibility distribution that remains consistent with $p$. This ensures that the resulting possibilistic model is as informative as possible.

A standard method that achieves this is based on preserving the preference ordering of the probabilities. Given a probability distribution $p$ over a finite universe $\Omega = \{\omega_1, \dots, \omega_n\}$, with elements ordered such that their probabilities are non-increasing ($p_1 \ge p_2 \ge \dots \ge p_n$), the transformation is defined as:
\[ \pi(\omega_k) = \sum_{i=k}^{n} p_i \]
The possibility of an outcome $\omega_k$ is the total probability mass of all outcomes that are equally or less probable than $\omega_k$. This construction guarantees consistency ($P(A) \le \Pi(A)$) and creates the tightest possible possibilistic envelope around the probability distribution \cite{Dubois1997}. This transformation is an act of summarization, trading the precision of an additive measure for a qualitative, less demanding representation of uncertainty.

\subsubsection{From Possibility to Probability ($\Pi\to$P): The Principle of Insufficient Reason}
Conversely, transforming a possibility distribution $\pi$ into a single probability distribution $p$ requires selecting one representative from the credal set $\mathcal{P}(\Pi)$. This involves an ``information filling-in" step, guided by the principle of insufficient reason or ``least commitment". The aim is to derive a probability measure that reflects the uncertainty encoded in $\pi$ without adding extraneous assumptions.

A well-known transformation, consistent with this principle, distributes the probability mass associated with each possibility level uniformly among the elements that attain it. Given distinct possibility levels $1 = \alpha_0 > \alpha_1 > \dots > \alpha_m > 0$, the probability mass $(\alpha_{j-1} - \alpha_j)$, which represents the ``drop" in confidence between levels, is shared equally among all elements in the $\alpha_{j-1}$ level-cut ($E_{j-1} = \{\omega : \pi(\omega) \ge \alpha_{j-1}\}$). The resulting probability for an element $\omega$ is the sum of all the shares it receives:
\[ p(\omega) = \sum_{j=1}^{m} \frac{\alpha_{j-1} - \alpha_j}{|E_{j-1}|} \cdot \mathbf{1}_{E_{j-1}}(\omega) \]
This method, also known as a pignistic transform in belief function theory, yields the center of gravity of the credal set $\mathcal{P}(\Pi)$ \cite{Dubois1997}. It produces a single, usable probability distribution by making the maximally non-committal assumption of uniformity wherever the possibility distribution allows for ambiguity.

It is important to note that the principles of maximum specificity and insufficient reason are not the only guides for such transformations. Other approaches exist, driven by different objectives. For instance, some methods aim to preserve information-theoretic quantities like entropy (as in the work of Klir \cite{Klir1990}) or to optimize a specific consistency index (as in Civanlar and Trussell \cite{Civanlar1986}). The choice of transformation ultimately depends on the intended application: whether the goal is a faithful logical approximation, a pragmatic basis for decision-making, or the preservation of specific mathematical properties across different uncertainty formalisms.