% \signal{Possibility is normalized so that 1 membership is attained at some point. And it models the compatibility of 2 states. That is of being 1.80 height and being tall states.

% Probability Models aleatoric uncertainty y Possibility models epistemic uncertainty?\\

% Probability-Possibility Transformations:
% Under certain conditions, one can associate a family of probability distributions with a given possibility distribution. For instance, the possibility distribution can be seen as an upper envelope for a set of probability measures that are consistent with the available imprecise information. Such transformations (e.g., the Dubois-Prade method) allow one to move between the two representations, albeit at the cost of introducing conservatism or ambiguity.\\


% Bayesian Models for modeling both uncertainties:\\

% Epistemic Uncertainty:

% Bayesian models explicitly account for uncertainty about the model parameters by placing a prior over them and computing a posterior after observing data. This kind of uncertainty reflects our lack of knowledge due to limited data and can be reduced by gathering more information.\\

% Aleatoric Uncertainty:

% This type of uncertainty represents the inherent noise in the data itself (for example, measurement error). In Bayesian modeling, aleatoric uncertainty is often incorporated into the likelihood function. It is considered irreducible since it stems from the randomness in the data generation process.\\

% Osea que a qué nos referimos con epistemologic uncertainty? A que nos falta info (data) o a que tenemos toda la info que se puede tener y no podemos reducir ese gap de incetidumbre porque es un problema de definición imprecisa. Tengo que diferenciar entre vagueza y epistemologico entonces?

% \subsection{Probability-Possibility Transformations}
% %https://www.researchgate.net/publication/2743789_On_PossibilityProbability_Transformations
% From the paper: \cite{Dubois1997}: 
% it is recalled in this paper that the probabilistic representations and the possibilisticones are not just two equivalent representations of uncertainty

% The possibilisticrepresentation is weaker because it explicitly handles imprecision (e.g. incompleteknowledge) and because possibility measures are based on an \textbf{ordering structure} rather than an \textbf{additive one}.

% the principle of insufficient reason from possibility toprobability, and the principle of maximum specificity from probability topossibility.

% Mencionar: consistency principle, principle of insufficient reason and principle of maximum specificity
% }






% \signal{Y HABLAR TAMBIÉN DE PLAUSIBILITY MEASURE Y CÓMO ESO SE RELACIONA CON EL LAS OTRAS MEASURES. MIRA EN Joseph Y. Halpern - Reasoning about Uncertainty-The MIT Press (2003)}


The study of fuzzy measures emerged as a generalization of classical measure theory, motivated by the increasing realization that the additivity axiom, fundamental in traditional measurement and probability theories as formalized by Borel, Lebesgue, and Kolmogorov, was often ``too restrictive and, consequently, unrealistic" for modeling real-world complexities \cite[p.~10]{FuzzyMeasureHistory}. An alternative to classical measure theory came with Choquet's theory of capacities in 1954, followed by Dempster's and Shafer's development of belief and plausibility measures\footnote{Belief and plausibility measures are defined using a ``basic probability assignment", which distributes a total measure of evidence across subsets $A_i \,(i\in \mathcal{I}\subseteq \N)$ of outcomes. When these subsets are ``consonant" (they can be ordered with inclusion: $A_i \subseteq  A_{i+1}\forall i\in \mathcal{I}$), these measures become equivalent to possibility and necessity measures, respectively. This makes possibility and necessity special, highly structured cases within the broader Dempster-Shafer framework \cite[Thm.~3.23, Thm.~3.25]{FuzzyMeasureHistory}.} (superadditive and subadditive functions) in the late 1960s and 1970s to handle interval-valued probabilities. Simultaneously, the concept of a fuzzy set, introduced by Zadeh in 1965, laid the groundwork for possibility measures. It was within this context of fuzzy sets that Sugeno, in 1974, conceived fuzzy measures and fuzzy integrals, replacing the strict additivity requirement with weaker axioms of monotonicity and continuity, thereby paving the way for a more nuanced mathematical framework capable of capturing intrinsic nonadditive aspects of real-world phenomena like measurement errors and subjective judgments \cite[p.~13]{FuzzyMeasureHistory}.\\

\begin{definition}[Fuzzy Measure (Capacity)]
Let $\Omega$ be a universal set. A fuzzy measure (or capacity) $\nu$ is a set function
\[ \nu: 2^\Omega \to [0, 1] \]
that assigns a value to each subset of $\Omega$ (often called an event or criterion) such that:
\begin{romanenum}
    \item \textbf{Boundary Conditions:} The measure of the empty set is $0$ ($\nu(\emptyset) = 0$) and the measure of the universal set is $1$ ($\nu(\Omega) = 1$).

    \item \textbf{Monotonicity:} For any $A, B \subseteq \Omega$, if $A \subseteq B$, then $\nu(A) \le \nu(B)$.
\end{romanenum}
\end{definition}

The value $\nu(A)$ may quantify the weight, importance, degree of belief, or capacity associated with the subset $A$. A relevant aspect of any fuzzy measure $\nu$ defined over a finite universal set $\Omega$, is that it is fundamentally defined by the values assigned to all the subsets (i.e.\ $2^{|\Omega|}$ values in total, minus the two fixed boundary values). This implies that the greater generality of fuzzy measures comes at the cost of an exponentially growing number of parameters. By contrast, an additive measure only requires a linearly growing number of parameters $|\Omega|$. 
This is the motivation behind k-additive or $\lambda$-fuzzy measures (not further studied in this work), which employ parametric approaches to reduce the degrees of freedom, achieving polynomial complexity in the size of the universal set.\\


Fuzzy measures can be categorized according to various properties. For the purposes of this work, the discussion is focused on the additivity properties, and their formal definitions are presented as follows:


\begin{definition}[Additivity]
Given disjoint sets $A,B \subseteq N$ (i.e., $A \cap B = \emptyset$), a fuzzy measure $\nu$ is:
\begin{romanenum}
    \item \textbf{Subadditive} if $\nu(A \cup B) \leq \nu(A) + \nu(B)$ \cite[Def. 2.10, Eq. 2.8]{beliakov2023discrete}
    \item \textbf{Additive} if $\nu(A \cup B) = \nu(A) + \nu(B)$ \cite[Def. 2.4, Eq. 2.1]{beliakov2023discrete}
    \item \textbf{Superadditive} if $\nu(A \cup B) \geq \nu(A) + \nu(B)$ \cite[Def. 2.10, Eq. 2.9]{beliakov2023discrete}
\end{romanenum}
\end{definition}

\begin{example}[Additivity of probability and possibility measures]
    Probability measures are additive for disjoint sets $A,B$, satisfying $P(A \cup B) = P(A) + P(B)$. On the other hand,possibility measures, are maxitive, satisfying $Pos(A \cup B) = \max\{Pos(A), Pos(B)\}$ for any sets $A,B$ (a property that, for possibility measures, holds even for non-disjoint sets). The maxitivity of possibility measures directly implies they are subadditive, since $\max\{Pos(A), Pos(B)\} \leq Pos(A) + Pos(B)$ and measures are non-negative.
\end{example}


The crucial departure from the additivity requiremen allows fuzzy measures to model situations where, for example, the combined importance of two criteria $A$ and $B$ might be greater than the sum of their individual importances (synergy) or less (redundancy). 



\subsection{Probability and Possibility Measures}
Probability and possibility measures are two distinct yet related formalisms for quantifying uncertainty, both falling under the umbrella of fuzzy measures.

\begin{definition}[Probability Measure]
A probability measure $P$ is a fuzzy measure that additionally satisfies the axiom of finite additivity for disjoint sets:
For any two disjoint sets $A, B \subseteq \Omega$ (i.e., $A \cap B = \emptyset$):
\[ P(A \cup B) = P(A) + P(B) \]
\end{definition}
A probability measure quantifies the likelihood or long-term frequency of an event occurring. 


\begin{definition}[Possibility Measure]
A possibility measure $\Pi$, pioneered by Zadeh \cite{Zadeh1978}, is a fuzzy measure characterized by the axiom of maxitivity:
For any two sets $A, B \subseteq \Omega$:
\[ \Pi(A \cup B) = \max(\Pi(A), \Pi(B)) \]
\end{definition}
A possibility measure quantifies the degree to which an event is considered feasible (meaning possible, plausible, or compatible with available knowledge). \\

Both measures are typically derived from an underlying distribution. A probability measure is built from a probability mass function $p$ via summation, $P(A) = \sum_{x \in A} p(x)$, and is normalized such that $\sum_{x \in \Omega} p(x) = 1$. A possibility measure is built from a possibility distribution $\pi$ via the supremum, $\Pi(A) = \sup_{x \in A} \pi(x)$, and is normalized such that $\sup_{x \in \Omega} \pi(x) = 1$.\\


When Zadeh introduced possibility theory, he understood the possibility of an event as an upper bound on its probability. This is formalized by the \textbf{consistency principle}, which states that the probability of an event cannot exceed its possibility, i.e. $p(\omega) \leq \pi(\omega) \forall \omega \in \Omega$. This extends the idea of ``an unfeasible event must be impossible'', to a weak connection between probabilities and possibilities. In situations where the true probability $p$ is unknown, this principle is useful for defining a set of all probability distributions that are compatible with our current knowledge. 

An illustrative example is presented to facilitate the understanding of both probability and possibility measures, as well as the distinction between possibilities and memberships:


\begin{example}[Probabilities, Possibilities, and Memberships: \textit{An Ugly Die}]
    
    Imagine a six-sided die where each face, $\omega \in \Omega = \{1, 2, \dots, 6\}$, has a different painting on it. We want to model this situation using three distinct numerical scales:
    
    \begin{enumerate}
        \item \textbf{Probability $p(\omega)$}: This represents the objective, long-run frequency of an outcome. In general, we do not know the probability distribution of a random experiment, but let us assume for the example that the die is \textit{actually} fair, so the probability of any face appearing is uniform.
        
        \item \textbf{Feasibility Possibility $\pi_F(\omega)$}: This captures our \textit{prior belief} about the die's behavior. Before rolling it, we handle the die and get the impression that face 1 feels slightly heavier. On a standard die, the opposite face is 6, so we believe it is more feasible (i.e., more likely based on our current knowledge) for face 6 to land up. We also think faces 1 and 6 are more plausible outcomes than the others, which all seem equally possible. This expresses our subjective epistemic uncertainty.
        
        \item \textbf{Beauty Membership $\mu_B(\omega)$}: This represents our subjective \textit{preference} or aesthetic judgment. It quantifies the degree to which each face's painting belongs to our own concept of a ``beautiful face".
    \end{enumerate}
    
    These three distinct concepts are summarized in the table below:
    
    \begin{center}
    \begin{tabular}{c|c|c|c}
        \textbf{Face ($\omega$)} & \textbf{Probability $p(\omega)$} & \textbf{Feasibility $\pi_F(\omega)$} & \textbf{Beauty $\mu_B(\omega)$} \\
        \hline
        1 & $1/6 \approx 0.167$ & 0.7 (Plausible) & 0.0 (Awful) \\
        2 & $1/6 \approx 0.167$ & 0.5 (Possible) & 0.0 (Awful) \\
        3 & $1/6 \approx 0.167$ & 0.5 (Possible) & 0.0 (Awful) \\
        4 & $1/6 \approx 0.167$ & 0.5 (Possible) & 0.0 (Awful) \\
        5 & $1/6 \approx 0.167$ & 0.5 (Possible) & 0.1 (A bit nice) \\
        6 & $1/6 \approx 0.167$ & 1.0 (Most feasible) & 1.0 (Beautiful) \\
    \end{tabular}
    \end{center}
    
    Then, \textit{what is the probability of getting a beautiful face?}
    
    To answer this, we must first define the event ``getting a beautiful face" as a crisp set over the set of outcomes, where our probability measure is defined. This crisp set depends on the \textbf{beauty membership} $\mu_B$. A natural way is to construct $B$ containing all faces that have a non-zero degree of beauty (any other desired threshold could be used):
    \[ B = \{\omega \in \Omega \mid \mu_B(\omega) > 0\} = \{5, 6\} \]
    The probability of this event is then calculated, as with any other event, by summing the probability masses of its elements:
    \[ P(B) = p(5) + p(6) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3} \]
    The only role of the membership function $\mu_B$ was to \textit{define the crisp event} $B$. The possibility distribution $\pi_F$ was entirely irrelevant for this specific calculation.
    
    \textit{Why use a possibility distribution at all?}

    As mentioned, a primary utility of a feasibility model like $\pi_F$ is to constrain the set of plausible probability distributions when the true probability is unknown, via the consistency principle $p(\omega) \leq \pi_F(\omega)$. However, this approach carries a significant risk: if the possibility distribution imposes bounds that are too restrictive (e.g., if we had mistakenly set $\pi_F(2)=0.1$), the resulting set of compatible probabilities could be completely unrealistic, or even exclude the true probability distribution altogether. For this reason, a conservative approach is generally preferred when modeling with possibilities. It is safer to define wider, less specific bounds to ensure that reality is not inadvertently ruled out by an overly confident or incorrect assumption.
\end{example}

A relevant question that arises from this example is: What is the difference between using a possibility or a subjective probability (e.g. Bayesian framework)? Two main distinctions can be identified: The possibility measure offers more expressiveness than a probability measure because it allows the representation of concepts such as absolute ignorance (where every subset has equal measure), which cannot be represented with an additive measure\cite{Dubois1997}. Additionally, a possibility measure do not specify the likelihood of any event (even though it can be used to derive a probability distribution, see the next subsection), making the direct update via Bayes rule illogical. Therefore, possibility distributions do not replace subjective probabilities, but rather provide a less restricted representation of our current knowledge from where a subjective probability could be derived.\\

\begin{remark}
    Not all the possibility distributions must be derived using only the subjective intuition of a decision maker as in the example. Data and more structured ``objective"\footnote{The objectivity of this approach is debatable since the selection of data and how it is processed can involve subjective choices despite being justifiable.} approaches could be used, or a mix of both. 
    For instance, the feasibility of a car making a sharp turn is based on physical limits: while completing the turn is fully possible ($\pi=1$) at 16~km/h, it becomes only partially possible at 64~km/h as the required centripetal force approaches the maximum friction of the tires, and physically impossible ($\pi=0$) at 97~km/h.
\end{remark}

Complementary to the possibility measure is its dual measure, called necessity measure, which quantifies the degree to which an event is not just feasible, but certain.

\begin{definition}[Necessity Measure]
A necessity measure $N$ is the dual of a possibility measure $\Pi$, and is defined for any set $A \subseteq \Omega$ by:
\[ N(A) = 1 - \Pi(A^c) \]
where $A^c$ is the complement of $A$.
\end{definition}

The intuition behind this formula can be understood by thinking about what it means for an event to be certain based on our knowledge. An event $A$ is absolutely certain only if its complement, $A^c$ (the event "not A"), is absolutely impossible (unfeasible). If there is any feasibility to "not A", our certainty in "A" must be diminished.

The term $\Pi(A^c)$ precisely captures this ``plausibility of doubt". It is the highest degree of support our knowledge gives to any outcome outside of $A$. In the context of the consistency principle from Zadeh, necessity is interpreted as a lower bound for the probability distribution:
\[N(A) \le P(A) \le \Pi(A)\]

\subsection{Probability-Possibility Transformations}

The relationship $N(A) \le P(A) \le \Pi(A)$ \signal{Mencionar los principios de los que salle esta desigualdad} suggests an ordering and provides a basis for transformations. Transformations between these measures are often wanted for practical reasons:
\begin{itemize}
    \item We might have probabilistic information but need a possibilistic model for certain types of reasoning (e.g., reasoning about feasibility under uncertainty).
    \item We might have possibilistic information (e.g., from expert elicitation of what's feasible) but need a probability distribution for decision-making processes that require expected utility calculations.
\end{itemize}
The core assumption underlying many transformations is that possibility acts as an upper bound for probability: $P(A) \le \Pi(A)$ for all $A \subseteq \Omega$. This implies that what is probable must first be possible. An event cannot be likely if it's not even deemed possible to some corresponding degree. The set of all probability measures $P$ consistent with a given possibility measure $\Pi$ is called a credal set.

\subsubsection{Transformations from Probability to Possibility (P$\to\Pi$)}
The goal here is to summarize or bound a precise probability distribution $p$ (defined on elementary events) with a possibility distribution $\pi$ (which induces $\Pi$) such that $p(x) \le \pi(x)$ for all $x \in \Omega$, and often, to preserve the ranking of probabilities.
One common method, attributed to Dubois and Prade \cite{Dubois1997}, is the least-specific dominating possibility distribution.
Given a probability distribution $p=(p_1, \dots, p_n)$ over a finite universe $\Omega = \{\omega_1, \dots, \omega_n\}$, with elements ordered such that $p_1 \ge p_2 \ge \dots \ge p_n$ (where $p_k$ is the probability of $\omega_k$). The possibility degree for $\omega_k$ is defined based on cumulative probabilities:
\[ \pi(\omega_k) = \sum_{i=k}^{n} p_i \]
This is typically normalized so that $\max_k \pi(\omega_k) = \pi(\omega_1) = 1$.
Intuition: The most probable element is fully possible. The possibility of less probable elements reflects the cumulative probability of them and all elements less probable than them. This ensures the order-preserving property and consistency with $P(A) \le \Pi(A)$. Such transformations inherently involve a loss of information, as the additivity of probability is replaced by the weaker maxitivity of possibility.

\subsubsection{Transformations from Possibility to Probability ($\Pi\to$P)}
The goal is to select a single probability distribution $p$ from the credal set $\mathcal{P}(\Pi)$ defined by a possibility distribution $\pi$. This often involves an ``information filling-in" step, guided by principles like maximum entropy or insufficient reason.
A well-known example is the pignistic transform (context of belief functions, of which possibility measures are a special case). Given distinct possibility levels (in general, a consonant belief) $1 = \alpha_0 > \alpha_1 > \dots > \alpha_m = 0$, we define $\alpha$-cuts $E_j = \{\omega : \pi(\omega) \ge \alpha_j\}$. The probability mass $(\alpha_{j-1} - \alpha_j)$ that ``drops" between two successive possibility levels is distributed uniformly among the elements in the narrower (more possible) set $E_{j-1}$:
\[ p(\omega) = \sum_{j=1}^{m} \frac{\alpha_{j-1} - \alpha_j}{|E_{j-1}|} \mathbf{1}_{\{\omega \in E_{j-1}\}} \]
Elements that are equally highly possible share the available probability mass assigned to that level of possibility. This transform selects the probability distribution that is maximally non-committal (maximizes entropy) within the constraints imposed by the possibility measure. These transformations add assumptions to select one specific probability distribution from many consistent ones.

\signal{Tengo que añadir las limitaciones de este tipo de transformaciones y referenciarlo todo esto a sus papers.}
% \subsubsection{Limitations and Assumptions of Transformations}
% It is crucial to understand that transformations are not universally applicable without careful consideration of their underlying assumptions:
% \begin{enumerate}
%     \item \textbf{The $P(A) \le \Pi(A)$ Postulate:} This is a fundamental modeling choice, not an inherent law. It is justified if the possibility measure truly represents available information about the upper bounds of potential probabilities (e.g., $\Pi$ derived from imprecise observations constraining $P$). It can be problematic if $P$ and $\Pi$ are derived from entirely independent sources or model fundamentally different aspects of a problem.
%     \item \textbf{Information Loss/Gain:} Transforming $P \to \Pi$ loses the precision of additivity. Transforming $\Pi \to P$ involves making assumptions (like maximum entropy or insufficient reason) to pick one $P$ out of a set of possibilities. The choice of transformation method itself is an assumption.
%     \item \textbf{Semantic Alignment:} Transformations are most meaningful when probability and possibility are indeed modeling different facets of the \textit{same underlying uncertainty}. If they model completely unrelated concepts, applying formal transformations can be a category mistake, as the numbers, despite being in $[0, 1]$, have entirely different semantics.
% \end{enumerate}