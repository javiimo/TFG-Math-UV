\signal{Explain the attributes and how to compute them. 
}\\
The presented optimization problem employs binary decision variables and a single aggregated objective function with crisp parameter values to provide a clear, tractable, and straightforward formulation. Such a representation is very practical due to its computational efficiency, interpretability, and ease of implementation using standard optimization solvers. However, this framework neglects the uncertainty, imprecision, and conflicting nature of  real-world criteria, such as varied stakeholder preferences, uncertain resource availability, or imprecise risk estimations. \\

However, this imprecise knowledge and information that the challenge providers have access to such as the physical location of each intervention or an interval for the possible risk values, is not public. To formulate our fuzzy MCDM problem, we will make minimal assumptions to extract meaningful information from the available data. These assumptions will help us reconstruct plausible attributes while maintaining the integrity of the original problem structure.\\

\signal{Another important aspect of a Multi-criteria decision making problem is having alternatives we want to choose from. In this case, since some of the algorithms that ranked among the top }

\subsection{Retriving the physical location of the interventions}
One of the assumptions that will be made is that there is a component of the risk that comes from the location of an intervention. This is intuitive since interventions in densely populated areas or near critical infrastructure likely carry higher risks than those in remote locations. However, location is not the only factor, the risk also depends on other variables like the type of maintenance work. \\

Therefore, we will attempt to infer the approximate location of the interventions stating the following:
\begin{notation}{Assumption 1:}
    Two interventions \(i\) and \(j\) are close, for all time periods, their overall mean risks (averaged accross all possible start times and scenarios)
    \[
    \overline{r}_t^{(i)} = \frac{1}{\left|\mathcal{T}_i\right|}\sum_{\mathcal{T}_i}\mu_t^{(i,\tau)}, 
    \qquad \text{where }\mu_t^{(i,\tau)} \coleq \frac{1}{|S|}\sum_{s\in S} \mathrm{risk}_{s,t}^{(i,\tau)}, \quad
    \mathcal{T}_i\coleq \{\tau\in \N:\,1\leq \tau \leq T-d_{i,\tau}\}
    \]
    vary in the same direction, i.e.,
    \[
    \operatorname{sgn}\Big(\overline{r}_{t+1}^{(i)}-\overline{r}_t^{(i)}\Big)=\operatorname{sgn}\Big(\overline{r}_{t+1}^{(j)}-\overline{r}_t^{(j)}\Big)\quad\forall t\in\{1,\dots , T\}
    \]
    This affirms that the risk variation of an intervention over time is strongly correlated with its location.
    \end{notation}


In practice, this crisp equivalence relation will rarely hold for all time periods $t$. However, we can use it to define a similarity measure between interventions by counting how often their risk variations align. Specifically, for each pair of interventions $(i,j)$, we compute a similarity score by adding 1 when their risk variations match in sign and subtracting 1 when they differ. Then it is normalized to the interval $[-1,1]$. This yields a symmetric similarity matrix that captures the degree of correlation between intervention risks over time.

\begin{remark}
    By definition, an intervention has a correlation of 1 with itself.
\end{remark}

Notice that not all correlation values provide information about the closeness between interventions. Only positively correlated interventions can be considered "close" to some extent. However, we cannot determine how far apart negatively correlated interventions should be. Therefore, these negative correlations will be set to \texttt{NaN} and treated as missing values.\\

There is not enough information to assign a specific distance value between 2 close points, but following assumption 1: the higher the correlation between two interventions, the stronger the indication that they are close to each other. Therefore, the algorithm proposed to recover a map from a distance matrix is: non-metric weighted Multi-Dimensional Scaling (MDS):

\begin{itemize}
    \item \textbf{MDS:} Given a distance matrix (or dissimilarity matrix), it minimizes a stress function (\ref{eq:stress}) to find an embedding in a specified number of dimensions (in our case, 2 dimensions).
    \item \textbf{Non-metric:} Instead of using the magnitude of the distances, it only considers the ordinal information induced by those values. Higher correlations indicate closer distances, but the size of the difference between correlations is not necessarily proportional to the actual distances between interventions.
    \item \textbf{Weighted:} Each distance's contribution to the stress function is weighted by its correlation value, reflecting higher confidence in our assumption when correlations are stronger.
\end{itemize}

\begin{remark}
    The dissimilarity/distance matrix is obtained through a linear transformation of the previously defined similarity matrix:
    \begin{align*}
        \text{NaN} &\rightarrow \text{NaN} \\
        \text{cor}_{ij} &\rightarrow \text{dist}_{ij} = 1-\text{cor}_{ij}, \quad \text{where } \text{cor}_{ij} \in [0,1]
    \end{align*}
\end{remark}


\subsubsection*{Non-metric Weighted MDS with SMACOF:}
\paragraph{Problem Formulation .} Let us assume we have $n$ objects, and an $n\times n$ matrix of observed dissimilarities 
\[
\Delta = \bigl[\delta_{ij}\bigr],
\]
where $\delta_{ij} \ge 0$ and each pair of objects $(i,j)$ is assigned a weight $w_{ij}\ge 0$. The goal of \emph{non-metric} weighted MDS is to find an embedding of these objects as points $\mathbf{x}_1,\dots,\mathbf{x}_n$ in $\mathbb{R}^p$ (collectively denoted by $X \in \mathbb{R}^{n\times p}$), such that the pairwise distances $d_{ij}(X) = \|\mathbf{x}_i - \mathbf{x}_j\|$ (usually Euclidean) respect the \emph{rank ordering} of $\delta_{ij}$.

Formally, non-metric MDS replaces $\delta_{ij}$ with a \emph{monotonic transformation} $f(\cdot)$, yielding
\[
\hat{\delta}_{ij} = f(\delta_{ij}),
\]
subject to the isotonic constraint:
\[
\delta_{ij} < \delta_{kl} 
\quad \Longrightarrow \quad 
f(\delta_{ij}) \;\le\; f(\delta_{kl}).
\]
We then define the \emph{weighted STRESS} function:
\begin{equation}
\mathrm{STRESS}(X,\hat{\Delta}) 
\;=\;
\sum_{i<j} w_{ij}\,\bigl(\hat{\delta}_{ij} \;-\; d_{ij}(X)\bigr)^2,
\label{eq:stress}
\end{equation}
where $\hat{\Delta}$ denotes the matrix $\bigl[\hat{\delta}_{ij}\bigr]$.  
The goal is to minimize this STRESS with respect to both $X$ and the (monotonic) mapping $f$. Missing values are simply not taken into account in the sum (which is equivalent to setting their weight to 0).

\paragraph{Solution via SMACOF}\label{sec:smacof-solution}
%Here explain that this is an iterative algorithm with 2 parts: isotonic regression, then majorization update, then isotonic regression, etc. Explain that the difference between Kruskal and SMACOF is that Kruskal uses gradient descent and SMACOF uses this majorization stuff and how that is better to handle missing values and monotony of the convergence.

\emph{Scaling by MAjorizing a COmplicated Function} is an iterative algorithm that provides a stable, monotone-convergent framework for MDS. Unlike Kruskal's classic method, which uses gradient-based updates to move the configuration $X$, SMACOF replaces the gradient step with a \emph{majorization step} that guarantees non-increasing STRESS accross iterations. This is typically more numerically stable and better at handling large amounts of missing data.

Concretely, SMACOF for non-metric weighted MDS proceeds in two main parts each iteration:
\begin{enumerate}
  \item \emph{Optimal scaling} via isotonic regression (enforcing rank preservation of the dissimilarities),
  \item \emph{Majorization update} to find $X^{(t+1)}$ with a guaranteed monotone decrease of STRESS.
\end{enumerate}


  \begin{enumerate}
    \item \textbf{Initialization}
      \begin{enumerate}[(a)]
          \item Collect all non-missing dissimilarities $\{\delta_{ij}\}$, sorting them in ascending order.
          \item Choose an initial configuration $X^{(0)}$ in $\mathbb{R}^p$ (e.g., by random placement or via classical MDS).
      \end{enumerate}

    \item \textbf{Optimal Scaling (Isotonic Regression)}
      \begin{enumerate}[(a)]
          \item Given the current distances $d_{ij}\bigl(X^{(t)}\bigr)$, fit a monotonic transformation $f$ by minimizing
          \[
            \sum_{i<j} w_{ij}\,\Bigl(f(\delta_{ij}) - d_{ij}\bigl(X^{(t)}\bigr)\Bigr)^2
          \]
          subject to $f$ being non-decreasing in $\delta_{ij}$.
          \item In practice, one applies \emph{weighted isotonic regression} (e.g., via the Pool Adjacent Violators Algorithm) to obtain $\hat{\delta}_{ij} = f(\delta_{ij})$.
      \end{enumerate}

    \item \textbf{Iterative Majorization Update}
      \begin{enumerate}[(a)]
          \item With $\hat{\delta}_{ij}$ fixed, construct a majorizing function $Q\bigl(X, X^{(t)}\bigr)$ such that
          \[
            \mathrm{STRESS}(X,\hat{\Delta}) \;\le\; Q\bigl(X, X^{(t)}\bigr),
            \quad
            \text{with equality if } X = X^{(t)}.
          \]
          \item Minimizing $Q$ w.r.t.\ $X$ (the “Guttman transform”) typically yields a closed-form solution:
          \[
            X^{(t+1)} = \mathbf{G}\bigl(X^{(t)}, \hat{\Delta}\bigr).
          \]
          \item By construction of $Q$, we have
          \[
            \mathrm{STRESS}\bigl(X^{(t+1)}, \hat{\Delta}\bigr) 
            \;\le\;
            \mathrm{STRESS}\bigl(X^{(t)}, \hat{\Delta}\bigr),
          \]
          ensuring a \emph{monotone decrease} in STRESS at each iteration.
      \end{enumerate}

    \item \textbf{Convergence Check}
      \begin{enumerate}[(a)]
          \item After updating $X^{(t+1)}$, recompute the STRESS. If the change is below a chosen tolerance or no improvement is found, stop. Otherwise, return to \textbf{Step 2}.
      \end{enumerate}
\end{enumerate}


\subsection{Crisp Attributes}
Highest Concurrency
Highest Risk % Uso esto? Porque realmente no hay una diferencia abismal. Y el máximo absoluto es el mismo en todas.
winter-like
summer-like
is-like

\subsection{Fuzzy Attributes}
Closeness Concurrency
Environmental Impact Concurrency
Risk Concurrency %Maybe fuzzy clustering in theese into 5? Quiero hacer un fuzzy clustering y combinarlo esto con la concurrencia crisp y propagar el fuzzy clustering a fuzzy concurrency
Size Concurrency %Maybe fuzzy clustering in theese into 5?
% But then fuzzy clusters become crisp when adding the concurrency, right?